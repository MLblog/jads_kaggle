{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "from preprocessing import load, process, preprocess_and_save\n",
    "from aggregation import ohe_explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "The file *preprocessing.py* contains function to load the JSON data, convert to Pandas DataFrames, and perform basic preprocessing (e.g., delete columns that are constant). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load(\"../data/train.csv\", nrows=100000)\n",
    "test = load(\"../data/test.csv\", nrows=10000)\n",
    "\n",
    "# preprocess_and_save(\"../data\", nrows_train=100000, nrows_test=10000)\n",
    "train, test = process(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save(\"../data\", nrows_train=100000, nrows_test=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categoricals with many values\n",
    "\n",
    "There are a number of categorical features with many different values. This is an issue, specifically in the cases where those categorical features are not ordinal, and therefore label encoding them does not make sense. The only choice we are left with, is OHE. However naively performing this step would add hundreds of columns for each original categorical feature - in the end yielding potentially thousands of super sparse features. What I would like to explore, is whether there exist **specific values** with predictive value significantly higher than average. For example, looking at the particular country might be a weak predictor. However there might be 5 specific countries with a huge revenue deviation from the average (and enough samples to consider this discrepancy statistically significant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = train['country']\n",
    "print(\"There are {} different countries in our dataset\".format(len(countries.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = {'target':['mean', 'count']}\n",
    "\n",
    "countries = train[[\"country\", \"target\"]].groupby(\"country\", as_index=False).agg(aggregations)\n",
    "countries.columns = [\"country\", \"targetMean\", \"occurenceCount\"]\n",
    "\n",
    "# Let's focus only on countries with multiple records to preserve some statistical significance\n",
    "keep = 10\n",
    "usual_countries = countries.sort_values(\"occurenceCount\", ascending=False).head(keep)\n",
    "\n",
    "global_average = train[\"target\"].mean()\n",
    "usual_countries[\"deviation\"] = usual_countries[\"targetMean\"] - global_average\n",
    "usual_countries.plot.bar(x=\"country\", y=\"deviation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USA seems quite different\n",
    "\n",
    "What we can find here is that USA is very different to any other country with significant sample count. What we can do with this information? We instead of using OHE to code every country in our dataset, we can probably get away with a single boolean column: **is this record coming from the USA?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = train['city']\n",
    "print(\"There are {} different cities in our dataset\".format(len(cities.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = {'target':['mean', 'count']}\n",
    "\n",
    "cities = train[[\"city\", \"target\"]].groupby(\"city\", as_index=False).agg(aggregations)\n",
    "cities.columns = [\"city\", \"targetMean\", \"occurenceCount\"]\n",
    "\n",
    "# Let's focus only on countries with multiple records to preserve some statistical significance\n",
    "keep = 10\n",
    "usual_cities = cities.sort_values(\"occurenceCount\", ascending=False).head(keep)\n",
    "\n",
    "global_average = train[\"target\"].mean()\n",
    "usual_cities[\"deviation\"] = usual_cities[\"targetMean\"] - global_average\n",
    "usual_cities.plot.bar(x=\"city\", y=\"deviation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about the cities?\n",
    "\n",
    "Here we can see some pretty strong deviations. However we need to note that the top ones come from US cities, so part of the variance these cities explain, is already included in the information that they belong to the USA. However their deviation is considerably higher than that of USA alone (1.0 vs 0.3) so including those columns might still be beneficial. The deviation we see is actually very distorted because of the USA outlier. Perhaps it would make more sense to only focus on the deviation from the average of non-USA cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_not_us = train[train['country'] != \"United States\"]\n",
    "train_us = train[train['country'] == \"United States\"]\n",
    "\n",
    "outside_us_avg = train_not_us['target'].mean()\n",
    "us_avg = train_us[\"target\"].mean()\n",
    "\n",
    "print(\"Average in US: {}\\nAverage outside US: {}\".format(us_avg, outside_us_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's then repeat out analysis but this time separatly for the pieces of data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = {'target':['mean', 'count']}\n",
    "\n",
    "# US case\n",
    "def city_deviation(df, title=\"Deviation per city\"):\n",
    "    cities = df[[\"city\", \"target\"]].groupby(\"city\", as_index=False).agg(aggregations)\n",
    "    cities.columns = [\"city\", \"targetMean\", \"occurenceCount\"]\n",
    "\n",
    "    # Let's focus only on countries with multiple records to preserve some statistical significance\n",
    "    keep = 10\n",
    "    usual_cities = cities.sort_values(\"occurenceCount\", ascending=False).head(keep)\n",
    "\n",
    "    global_average = df[\"target\"].mean()\n",
    "    usual_cities[\"deviation\"] = usual_cities[\"targetMean\"] - global_average\n",
    "    ax = usual_cities.plot.bar(x=\"city\", y=\"deviation\", title=title, rot=45, legend=False)\n",
    "    ax.set_xlabel(\"City\")\n",
    "    ax.set_ylabel(\"Deviation\")\n",
    "    \n",
    "city_deviation(train_us, title=\"Deviation from the mean - US\")\n",
    "city_deviation(train_not_us, title=\"Deviation from the mean - Rest of the world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much better!\n",
    "\n",
    "Now we can clearly see what information should be included besides the country (or to be exact, whether or not the country is the US). For example it makes no sense to include Los Angeles or Mountain View even though they deviate from the global average, because all this deviation is explained by the fact that they exist in the US! Instead we should include Chicago, New York, Austin, Seattle and maybe Palo Alto. And as we can see the deviations are much smaller outside the US, with the exception of Toronto which MUST be included.\n",
    "\n",
    "### Food for thought\n",
    "It makes sense that deviations outside the US are smaller because the target itself is considerable lower. Perhaps we should look at relative deviations instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = ohe_explicit(train)\n",
    "check.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
