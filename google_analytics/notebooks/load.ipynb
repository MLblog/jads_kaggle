{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path, nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'},\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    # Normalize JSON columns\n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = pd.io.json.json_normalize(df[column])\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    \n",
    "    # Parse date\n",
    "    df['date'] = df['date'].apply(lambda x: pd.datetime.strptime(str(x), '%Y%m%d'))\n",
    "    print(\"Loaded file {}\\nShape is: {}\".format(path, df.shape))\n",
    "    return df\n",
    "\n",
    "def process(train, test):\n",
    "    print(\"Dropping constant columns...\")\n",
    "    \n",
    "    # Remove columns with constant values.\n",
    "    const_cols = [c for c in train.columns if train[c].nunique(dropna=False) == 1]\n",
    "    train = train.drop(const_cols, axis=1)\n",
    "    test = test.drop(const_cols, axis=1)\n",
    "    \n",
    "    train_len = train.shape[0]\n",
    "    merged = pd.concat([train, test], sort=False)\n",
    "\n",
    "    # Create some features.\n",
    "    merged['diff_visitId_time'] = merged['visitId'] - merged['visitStartTime']\n",
    "    merged['diff_visitId_time'] = (merged['diff_visitId_time'] != 0).astype(int)\n",
    "    del merged['visitId']\n",
    "    del merged['sessionId']\n",
    "\n",
    "    print(\"Generating date columns...\")\n",
    "    merged['WoY'] = merged['date'].apply(lambda x: x.isocalendar()[1])\n",
    "    merged['month'] = merged['date'].apply(lambda x: x.month)\n",
    "    merged['quarterMonth'] = merged['date'].apply(lambda x: x.day // 8)\n",
    "    merged['weekday'] = merged['date'].apply(lambda x: x.weekday())\n",
    "    del merged['date']\n",
    "\n",
    "    format_time = lambda t: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n",
    "    merged['visitHour'] = pd.to_datetime(merged['visitStartTime'].apply(format_time)).apply(lambda t: t.hour)\n",
    "    del merged['visitStartTime']\n",
    "    \n",
    "    print(\"Finding total visits...\")\n",
    "    # This could be considered an information leak as I am including information about the future when predicting\n",
    "    # the revenue of a transaction. In reality, when looking at the 3rd visit we would have no way of knowning\n",
    "    # that the user will actually shop X more times (or if he will visit again at all). However since this\n",
    "    # info also exists in the test set we might use it.\n",
    "    total_visits = merged[[\"fullVisitorId\", \"visitNumber\"]].groupby(\"fullVisitorId\", as_index=False).max()\n",
    "    total_visits.rename(columns={\"visitNumber\": \"totalVisits\"}, inplace=True)\n",
    "    merged = merged.merge(total_visits)\n",
    "\n",
    "    print(\"Splitting back...\")\n",
    "    train = merged[:train_len]\n",
    "    test = merged[train_len:]\n",
    "    return train, test\n",
    "\n",
    "def preprocess_and_save(data_dir):\n",
    "    train = load(os.path.join(data_dir, \"train.csv\"))\n",
    "    test = load(os.path.join(data_dir, \"test.csv\"))\n",
    "\n",
    "    target = train['transactionRevenue'].fillna(0).astype(float)\n",
    "    train['target'] = target.apply(lambda x: np.log1p(x))\n",
    "    del train['transactionRevenue']\n",
    "\n",
    "    train, test = process(train, test)\n",
    "    train.to_csv(os.path.join(data_dir, \"preprocessed_train.csv\"), index=False)\n",
    "    test.to_csv(os.path.join(data_dir, \"preprocessed_test.csv\"), index=False)    \n",
    "    \n",
    "\n",
    "# Call this to save the preprocessed data for later use\n",
    "# preprocess_and_save(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load(\"../data/train.csv\", nrows=15000)\n",
    "test = load(\"../data/test.csv\", nrows=10000)\n",
    "\n",
    "train, test = process(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
