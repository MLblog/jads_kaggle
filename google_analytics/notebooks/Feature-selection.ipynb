{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in general two reasons why feature selection is used:\n",
    "1. Reducing the number of features, to reduce overfitting and improve the generalization of models.\n",
    "2. To gain a better understanding of the features and their relationship to the response variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Exploring the data, removing redudant features and benchmarking](#1)\n",
    "2. [Correlation and feature selection](#2)\n",
    "3. [Univariate Statistics](#3)\n",
    "4. [Model Based Feature Selection using the Random Forrest Classifier](#4)\n",
    "5. [Model Based Feature Selection using LightGBM Classifier](#5)\n",
    "6. [Iterative Feature Selection](#6)\n",
    "7. [Recursive feature elimination with cross validation and random forest classification](#7)\n",
    "8. [PCA](#8)\n",
    "9. [Conclusions](#9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile, SelectFromModel, RFE\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.insert(0, '../data/')\n",
    "sys.path.insert(0, '../')\n",
    "from feature_selector import FeatureSelector\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from utils import plot_confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_SEED=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some function that will be used throughout the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(estimator, X_train, y_train, X_test, y_test, X_transformed=None, X_test_transformed=None):\n",
    "    \"\"\"\n",
    "    Prints the score of the original and the transformed data.\n",
    "    Returns the trained (in the transformed data) estimator.\n",
    "    \"\"\"\n",
    "    #Create clones/copies of the estimator\n",
    "    est_clone1=clone(estimator)\n",
    "    est_clone2=clone(estimator)\n",
    "    #Train the first clone at the original dataset\n",
    "    est_clone1.fit(X_train, y_train)\n",
    "    print(\"Score with all features: {:.3f}\".format(est_clone1.score(X_test, y_test)))\n",
    "    if all(v is not None for v in [X_transformed, X_test_transformed]):\n",
    "        #Train the second clone at the transformed dataset\n",
    "        est_clone2.fit(X_transformed, y_train)\n",
    "        print(\"Score with only selected features: {:.3f}\".format(est_clone2.score(X_test_transformed, y_test)))\n",
    "        return est_clone2\n",
    "def selected_columns(estimator, X_train):\n",
    "    \"\"\"\n",
    "    Returns an array with the features selected by the method used.\n",
    "    \"\"\"\n",
    "    mask = estimator.get_support()\n",
    "    columns = np.asarray(X_train.columns.values)\n",
    "    selected= np.asarray(mask)\n",
    "    columns_selected= columns[selected]\n",
    "    return columns_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is very big and for that reason I will use a chunk out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 50000\n",
    "data_dir = \"../data/\"\n",
    "train_file_name ='aggregated_train.csv'\n",
    "train_path = os.path.join(data_dir, train_file_name)\n",
    "df_train = pd.read_csv(train_path, nrows= chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an extra column where 0 is when a visitor has zero sum and 1 else\n",
    "df_train['label'] = np.where(df_train['target_sum']==0, 0, 1)\n",
    "df_train.drop(['target_sum'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size =0.2\n",
    "X = df_train.copy()\n",
    "y = df_train.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distirbution of the features for the X set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"label\", data=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are good and ready to move on ðŸ¤˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should remove the feature 'fullVisitorId' because we really want not to play any importnat role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['fullVisitorId', 'label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the data, removing redudant features and benchmarking <a name=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `feature_selector` `class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureSelector(data = X , labels = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of features with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.identify_missing(missing_threshold=0.0)\n",
    "missing_features = fs.ops['missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_features = fs.ops['missing']\n",
    "missing_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment we will remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(missing_features, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the features that have a single unique value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.identify_single_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_unique = fs.ops['single_unique']\n",
    "single_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.plot_unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.unique_stats.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking: Let's see the score with all our features and without any transformation. The score can be used as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state=RANDOM_SEED, test_size = test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "score(lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, classes=['Did not Buy', 'Buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=[\"Did not buy\", \"Buy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correlation and feature selection <a name=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method finds pairs of collinear features based on the Pearson correlation coefficient. For each pair above the specified threshold (in terms of absolute value), it identifies one of the variables to be removed. We need to pass in a correlation_threshold.\n",
    "\n",
    "This method is based on code found at https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "\n",
    "For each pair, the feature that will be removed is the one that comes last in terms of the column ordering in the dataframe. (This method does not one-hot encode the data beforehand unless one_hot=True. Therefore correlations are only calculated between numeric columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.identify_collinear(correlation_threshold=0.98)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many features are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = fs.ops['collinear']\n",
    "correlated_features[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view a heatmap of the correlations above the threhold. The features which will be dropped are on the x-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.plot_collinear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that helpful! But we view the details of the corelations above the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.record_collinear.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well we can expect that he operating system is highly correlated with the browser. I checked most of them and there is nothing important to see. But I set also very high the threshold. Maybe with lower threshold there will be something importnt to note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Univariate Statistics <a name=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background** : In univariate statistics, we compute whether there is a statistically significant relationship between each feature and the target. Here the target is the label.  Then the features that are related with the highest confidence are selected. In the case of classification, this is also known as analysis of variance (ANOVA). \n",
    "\n",
    "One score is computed for the first feature, and another score is computed for the second feature. But it does not indicate anything on the combination of both features (mutual information). This is the **main weakness** of F-score. Scikit uses as defalult the Anova f-value. The larger theF-score is, the more likely this feature is more discriminative. Therefore, we use this score as a feature selection criterion. In other words, F-score reveals the discriminative power of each feature independently from others.\n",
    "\n",
    "All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_uni = SelectPercentile(percentile=10)\n",
    "select_uni.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_uni = select_uni.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_uni.shape))\n",
    "# transform test set\n",
    "X_test_uni = select_uni.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The space of the data has been reduces significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the performance of logistic regression on all features against the performance using only the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test)))\n",
    "lr.fit(X_train_uni, y_train)\n",
    "print(\"Score with only selected features: {:.3f}\".format(lr.score(X_test_uni, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the dataset\n",
    "lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "lr = score(lr, X_train, y_train, X_test, y_test, X_train_uni, X_test_uni)\n",
    "#Make predictions for the test set\n",
    "y_pred = lr.predict(X_test_uni)\n",
    "#Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, classes=['Did not Buy', 'Buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test_uni)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, classes=['Did not Buy', 'Buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot classification report\n",
    "print(classification_report(y_test,y_pred,\n",
    "target_names=[\"Did not buy\", \"Buy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can observe that there is no significant difference between the scores of the original and reduced data and he/she is right. However we have significantly reduced the space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Based using RF <a name=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**: Model-based feature selection uses a supervised machine learning model to judge the importance of each feature, and keeps only the most important ones. The feature selection model needs to provide some measure of importance for each feature, so that they can be ranked by this measure.The SelectFromModel class is a meta-learner that selects all features that have an importance measure of the feature (based on the weights of the classifier) greater than the provided threshold (here median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_RF = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED), threshold='mean')\n",
    "select_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_RF = select_RF.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_RF.shape))\n",
    "# transform test set\n",
    "X_test_RF = select_RF.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the method\n",
    "lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "lr = score(lr, X_train, y_train, X_test, y_test, X_train_RF, X_test_RF)\n",
    "#Make predictions for the test set\n",
    "y_pred = lr.predict(X_test_RF)\n",
    "#Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, classes=['Did not Buy', 'Buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=[\"Did not buy\", \"Buy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the 10 most important features based on the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmr = 10\n",
    "features = selected_columns(select_RF, X_train)\n",
    "importances = select_RF.estimator_.feature_importances_\n",
    "topten = sorted(importances, reverse=True)[:nmr]\n",
    "ind = np.argpartition(importances, -nmr)[-nmr:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(nmr), importances[ind], color=\"b\", align=\"center\")\n",
    "plt.xticks(range(nmr), X_train.columns[ind],rotation=90)\n",
    "plt.xlim([-1, nmr])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model based  feature selection with LightGBM <a name=\"5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.identify_zero_importance(task = 'classification', eval_metric = '', \n",
    "                            n_iterations = 10, early_stopping = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First we can access the list of features with zero importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_importance_features = fs.ops['zero_importance']\n",
    "zero_importance_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold = 0.99 will tell us the number of features needed to account for 99% of the total importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.plot_feature_importances(threshold = 0.99, plot_n = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.feature_importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Importance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.identify_low_importance(cumulative_importance = 0.99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low importance features to remove are those that do not contribute to the specified cumulative importance. These are also available in the ops dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_importance_features = fs.ops['low_importance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_LightGBM = X_train.drop(low_importance_features, axis=1)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_LightGBM.shape))\n",
    "# transform test set\n",
    "X_test_LightGBM = X_test.drop(low_importance_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "score(lr, X_train_LightGBM, y_train, X_test_LightGBM, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred, target_names=[\"Did not buy\", \"Buy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Iterative Feature Selection <a name=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**: Recursive feature elimination is based on the idea to recursively remove features, build a model using the remaining attributes and calculates model accuracy. This process is applied until all features in the dataset are exhausted. Features are then ranked according to when they were eliminated. As such, it is a greedy optimization for finding the best performing subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_RFE = RFE(RandomForestClassifier(n_estimators=50, random_state=RANDOM_SEED), step = 50, n_features_to_select=200, verbose=1)\n",
    "select_RFE.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_RFE= select_RFE.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_RFE.shape))\n",
    "# transform test data\n",
    "X_test_RFE = select_RFE.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the method\n",
    "lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "score(lr, X_train, y_train, X_test, y_test, X_train_RFE, X_test_RFE)\n",
    "#Make predictions for the test set\n",
    "y_pred = lr.predict(X_test_RFE)\n",
    "#Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, classes=['Did not Buy', 'Buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred,\n",
    "target_names=[\"Did not buy\", \"Buy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recursive feature elimination with cross validation and random forest classification <a name=\"7\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**: RFE with cross validation starts with all the *n* features, makes predictions with cross validation using the classifier (here RF), computes the relative cross-validated performance score (here accuracy) and the ranking of the importance of the features. Then it eliminates the lowest *k* features in the ranking and re-makes the predictions, the computation of the performance score and the feature ranking. It proceeds until all the features are eliminated. Finally it outputs the set of features which produced the predictor with the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "estimator = RandomForestClassifier() \n",
    "select_RFECV = RFECV(estimator=estimator, step=400, verbose = 1, cv=5, scoring='accuracy')   #5-fold cross-validation\n",
    "select_RFECV = select_RFECV.fit(X_train, y_train)\n",
    "\n",
    "print('Optimal number of features :', select_RFECV.n_features_)\n",
    "print('Best features :', X_train.columns[select_RFECV.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_RFECV= select_RFECV.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_RFECV.shape))\n",
    "# transform test set\n",
    "X_test_RFECV = select_RFECV.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the method\n",
    "lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "lr = score(lr, X_train, y_train, X_test, y_test, X_train_RFECV, X_test_RFECV)\n",
    "#Make predictions for the test set\n",
    "y_pred = lr.predict(X_test_RFECV)\n",
    "#Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, classes=['Did not Buy', 'Buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=[\"Did not buy\", \"Buy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PCA  <a name=\"8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('pca', pca)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the number of components that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train)\n",
    "plt.figure(1, figsize=(14, 13))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_ratio_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see 800 components is a reasonable number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=800\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=n_components)\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('pca', pca)])\n",
    "# transform data onto number of selected principal components\n",
    "X_train_pca = pipe.fit_transform(X_train)\n",
    "print(\"Original shape: {}\".format(str(X_train.shape)))\n",
    "print(\"Reduced shape: {}\".format(str(X_train_pca.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the test set\n",
    "X_test_pca = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the method\n",
    "lr = LogisticRegression(random_state=RANDOM_SEED)\n",
    "lr = score(lr, X_train, y_train, X_test, y_test, X_train_pca, X_test_pca)\n",
    "#Make predictions for the test set\n",
    "y_pred = lr.predict(X_test_pca)\n",
    "#Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, classes=['Did not Buy', 'Buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=[\"Did not buy\", \"Buy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions <a name=\"9\"></a>\n",
    "In general, we could not perform better than the baseline in the classification task. However, we managed to achieve more or less the same results with way reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
