{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation with dynamic features\n",
    "This notebook shows how to do the aggregation with monthly values for certain columns. \n",
    "\n",
    "__Remark:__ Because we now filter out a lot of users that only visited once, this notebook is not such a pain in the ass anymore. Don't be afraid to run it, your memory will be sufficient and you'll be done in a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "from preprocessing import *\n",
    "from aggregation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing a huge file\n",
    "\n",
    "Below is the new version of `load`, where processing takes place in chunks. After all chunks have been processed, they are concatenated to a single file. Since many columns are either dropped or aggregated, the resulting dataframe fits in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run these the first time - after it you can just load the reduced_datasets.\n",
    "reduce_df(\"../data/train_v2.csv\", output=\"../data/reduced_train.csv\", nrows=None, chunksize=20000)\n",
    "reduce_df(\"../data/test_v2.csv\", output=\"../data/reduced_test.csv\", nrows=None, chunksize=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [

    "# print(\"Let's widen the train dataset.\")\n",
    "train = pd.read_csv(\"../data/reduced_train.csv\", dtype={'fullVisitorId': 'str'})\n",
    "wide_train = aggregate_only(train)\n",

    "wide_train.to_csv(\"../data/wide_train.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "print(\"Let's widen the test dataset.\")\n",
    "test = pd.read_csv(\"../data/reduced_test.csv\", dtype={'fullVisitorId': 'str'})\n",

    "wide_test = aggregate_only(test)\n",

    "wide_test.to_csv(\"../data/wide_test.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the dataset with the static variables as HOE merged with the dynamic variables\n",
    "\n"
   ]
  },
  {

   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],

   "source": [
    "# load external data\n",
    "\n",
    "train_e = pd.read_csv(\"../data/wide_train_with_external_data.csv\", dtype={'fullVisitorId': 'str'}) \n",
    "test_e = pd.read_csv(\"../data/wide_test_with_external_data.csv\", dtype={'fullVisitorId': 'str'}) \n",
    "def aggregate_data(train_e, test_e):\n",
    "    names = ['fullVisitorId', 'GDP', 'HDI']\n",
    "    train_e = train_e[names]\n",
    "    test_e = test_e[names]\n",
    "    merged_e = pd.concat([train_e, test_e], sort=False)\n",
    "    def trans_to_float(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    merged_e['HDI'] = merged_e['HDI'].apply(lambda x: trans_to_float(x))\n",
    "    print(merged_e.columns)\n",
    "    print(merged_e.dtypes)\n",
    "    merged_e =  merged_e.groupby(\"fullVisitorId\", as_index = False).mean()\n",
    "    print(merged_e.columns)\n",
    "    return merged_e\n",
    "\n",
    "external_data = aggregate_data(train_e, test_e)\n",
    "external_data.to_pickle('../data/external_data.pkl')\n",
    "train_e, test_e = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [

    "def stack_google(google, test, date_from=('16-10-2018')):\n",
    "    print('Adding the Google columns')\n",
    "    google['date'] = google['date'].apply(lambda x: pd.datetime.strptime(x, '%Y-%m-%d'))\n",
    "    test['date'] = test['date'].apply(lambda x: pd.datetime.strptime(x, '%Y-%m-%d'))\n",
    "    google = google[google['date']>=date_from]\n",
    "    google.columns = ['Unnamed: 0', 'Client ID', 'sessions', 'Avg. Session Duration',\n",
    "       'Bounce Rate', 'transactions', 'date', 'transactionRevenue', 'fullVisitorId',\n",
    "       'date_month', 'date_week']\n",
    "                 \n",
    "    for iter_ in test.columns:\n",
    "        try:\n",
    "            a = google[iter_]\n",
    "        except:\n",
    "            print(iter_)\n",
    "            google[iter_] = 0\n",
    "    \n",
    "    google = google[test.columns]\n",
    "    return pd.concat([test, google], sort=False)\n",

    "\n",
    "def merge_google(df, google):\n",
    "        def to_date(x):\n",
    "            try:\n",
    "                return pd.datetime.strptime(x, '%Y-%m-%d')\n",
    "            except TypeError:\n",
    "                return x\n",
    "            \n",
    "        google['date'] = google['date'].apply(lambda x: to_date(x))\n",
    "        df['date'] = df['date'].apply(lambda x: to_date(x))\n",
    "        google = google[['fullVisitorId', 'date', 'Avg. Session Duration', 'Bounce Rate']]\n",
    "        return df.merge(google, on=['fullVisitorId', 'date'], how ='left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/reduced_test.csv\", dtype={'fullVisitorId': 'str'})\n",
    "train = pd.read_csv(\"../data/reduced_train.csv\", dtype={'fullVisitorId': 'str'})\n",
    "google = pd.read_csv(\"../data/google_analytics.csv\", dtype={'fullVisitorId': 'str'})\n",
    "\n",
    "test = stack_google(google, test)\n",
    "    \n",
    "print('Merging the Google columns')\n",
    "train = merge_google(train, google)\n",
    "test = merge_google(test, google)\n",
    "\n",
    "\n",
    "train.to_pickle('../data/train.pkl')\n",
    "test.to_pickle('../data/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's merge the datasets\n",
    "train = pd.read_pickle(\"../data/train.pkl\")\n",
    "test = pd.read_pickle(\"../data/test.pkl\")\n",
    "\n",
    "merged  = deal_static(train, \n",
    "                      test, \n",
    "                      selec_top_per=0.5,\n",
    "                      max_cat=10)\n",
    "\n",
    "train, test = None, None  # To liberate memory space\n",
    "\n",
    "merged.to_pickle(\"../data/merged.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the date period!\n",
    "\n",
    "Once you have the merged dataset, you only need to run this chunck of code and change the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.read_pickle(\"../data/merged.pkl\")\n",
    "external_data = pd.read_pickle('../data/external_data.pkl')\n",
    "\n",
    "merged = merged.merge(external_data, how='left', on = 'fullVisitorId')\n",
    "# Date intervals to split the data\n",
    "x_train_dates=('2016-08-01', '2017-11-30') \n",
    "y_train_dates=('2017-12-01', '2018-01-31')\n",
    "x_test_dates=('2017-08-01', '2018-11-30')\n",
    "\n",
    "# Final data processing\n",
    "x_train, y_train, x_test, y_test = create_train_test(merged, \n",
    "                                                     x_train_dates=x_train_dates, \n",
    "                                                     y_train_dates=y_train_dates, \n",
    "                                                     x_test_dates=x_test_dates, \n",
    "                                                     y_test_dates=None)\n",
    "\n",
    "# Save dfs as pickle objects -> faster to load and save. In addition, we do not need to worry about format issues\n",
    "x_train.to_pickle(\"../data/x_train.pkl\") \n",
    "y_train.to_pickle(\"../data/y_train.pkl\") \n",
    "x_test.to_pickle(\"../data/x_test.pkl\")\n",
    "\n",
    "if y_test is not None:\n",
    "    y_test.to_pickle(\"../data/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first attempt!\n",
    "Let's see if we can fit a model on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the one-hot encoding, the columns of train and test are not the same. For this experiment, only keep the intersection of columns. There are also other ways to deal with this (e.g., by mapping categories to external data), so we don't do this in the aggregation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_pickle(\"../data/x_train.pkl\") \n",
    "y_train = pd.read_pickle(\"../data/y_train.pkl\") \n",
    "x_test = pd.read_pickle(\"../data/x_test.pkl\") \n",
    "\n",
    "# Save the dataset ids\n",
    "id_x_train =  x_train['fullVisitorId']\n",
    "id_y_train = y_train['fullVisitorId']\n",
    "id_x_test = x_test['fullVisitorId']\n",
    "\n",
    "# Delete fullVisitor ID -> probably we want to leave it as a OHE feature\n",
    "del x_train['fullVisitorId']\n",
    "del y_train['fullVisitorId']\n",
    "del x_test['fullVisitorId']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NaNs to zero and fit linear model\n",
    "from sklearn import linear_model\n",
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)\n",
    "y_train = y_train.fillna(0)\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "r_squared = lm.score(x_train, y_train)\n",

    "print(\"The model has an R^2 of {}.\".format(r_squared))\n",
    "# do the prediction\n",
    "prediction = list(lm.predict(x_test).flat)"

   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
