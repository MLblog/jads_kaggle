{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation with dynamic features\n",
    "This notebook shows how to do the aggregation with monthly values for certain columns. \n",
    "\n",
    "__Remark:__ Because we now filter out a lot of users that only visited once, this notebook is not such a pain in the ass anymore. Don't be afraid to run it, your memory will be sufficient and you'll be done in a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "from preprocessing import *\n",
    "from aggregation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the data\n",
    "First we need to load and preprocess the original data.\n",
    "Note that there might be three additional columns in the new dataset. These need to be preprocessed as well.\n",
    "\n",
    "The `preprocess_and_save` method now has an argument `drop_users=True` (default is True). You can set this to false if you wish to keep all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save(\"../data\", nrows_train=None, nrows_test=None, start_x_train='2016-08-01', \n",
    "                    end_x_train='2016-10-16', start_y_train='2016-12-01', end_y_train='2017-02-01', \n",
    "                    start_x_test='2017-08-01', end_x_test='2017-10-16', drop_users=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating data per customer\n",
    "\n",
    "We need to predict the target at customer-level, i.e., we predict one value for each customer\n",
    "in the test set. Our data, however, contains a row for every site visit. One obvious way to deal\n",
    "with this discrepancy is to aggregate the visit data by customer, to obtain features on the\n",
    "customer-level.\n",
    "\n",
    "Another potential problem of our dataset, in some cases, is the huge amount of categories.\n",
    "This Situation may lead to a dimensionality problem. One way to deal with this is to select \n",
    "the most 'promising' categories. Here, we  can select the number of categories that represent most of the sales by\n",
    "defining the arguments selec_top_per and max_cat.\n",
    "\n",
    "\n",
    "The _preprocessing.py_ file contains functions to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "x_train, y_train, x_test = load_train_test_dataframes(data_dir, nrows_train=None, nrows_test=None, selec_top_per=0.5, max_cat=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This only takes a few seconds now.\")\n",
    "x_train_aggregated = aggregate_data_per_customer(x_train, startdate_y='2016-12-01', startdate_x='2016-08-01')\n",
    "print(\"Train data is aggregated\")\n",
    "print(\"Aggregating test data.\")\n",
    "x_test_aggregated = aggregate_data_per_customer(x_test, startdate_y='2017-12-01', startdate_x='2017-08-01')\n",
    "print(\"Test data is aggregated\")\n",
    "y_train_aggregated = y_train.groupby(['fullVisitorId'])[['target']].sum()\n",
    "print(\"Train target data is aggregated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data should be close to what we need to start fitting models. We tried to keep as much information as possible, so from here it is of course still possible to remove features that seem unnecessary or do other dimensionality reduction. The good thing is that we don't have to do aggregation every time, so let's save the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving files\")\n",
    "x_train_aggregated.to_csv(os.path.join(data_dir, \"aggregated_x_train.csv\"), index=True)\n",
    "x_test_aggregated.to_csv(os.path.join(data_dir, \"aggregated_x_test.csv\"), index=True)\n",
    "y_train_aggregated.to_csv(os.path.join(data_dir, \"aggregated_y_train.csv\"), index=True)\n",
    "print(\"Aggregated data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first attempt!\n",
    "Let's see if we can fit a model on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the one-hot encoding, the columns of train and test are not the same. For this experiment, only keep the intersection of columns. There are also other ways to deal with this (e.g., by mapping categories to external data), so we don't do this in the aggregation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for illustration, so let's keep it simple\n",
    "from sklearn import linear_model\n",
    "\n",
    "# create train and test sets and labels excluding visitor ID\n",
    "x_train, x_test = keep_intersection_of_columns(aggregated_train.reset_index(drop=True),\n",
    "                                               aggregated_test.reset_index(drop=True))\n",
    "y_train = np.log(aggregated_train.reset_index(drop=True)[\"target_sum\"]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NaNs to zero and fit linear model\n",
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "r_squared = lm.score(x_train, y_train)\n",
    "print(\"The model has an R^2 of {}.\".format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and create a submission\n",
    "predictions = lm.predict(x_test)\n",
    "submission = pd.concat([aggregated_test.reset_index()[\"fullVisitorId\"], pd.Series(predictions)], axis=1)\n",
    "submission.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n",
    "\n",
    "# set everything below $1 to zero\n",
    "submission[\"PredictedLogRevenue\"] = np.maximum(0, submission[\"PredictedLogRevenue\"])\n",
    "submission[\"PredictedLogRevenue\"][submission[\"PredictedLogRevenue\"]<1] = 0\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"first_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
