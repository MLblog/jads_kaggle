{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Aggregation with dynamic features\n",
    "This notebook shows how to do the aggregation with monthly values for certain columns. \n",
    "\n",
    "__Remark:__ Because we now filter out a lot of users that only visited once, this notebook is not such a pain in the ass anymore. Don't be afraid to run it, your memory will be sufficient and you'll be done in a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "from preprocessing import *\n",
    "from aggregation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Managing a huge file\n",
    "\n",
    "Below is the new version of `load`, where processing takes place in chunks. After all chunks have been processed, they are concatenated to a single file. Since many columns are either dropped or aggregated, the resulting dataframe fits in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Only run these the first time - after it you can just load the reduced_datasets.\n",
    "reduce_df(\"../data/train_v2.csv\", output=\"../data/reduced_train.csv\", nrows=None, chunks ize=20000)\n",
    "reduce_df(\"../data/test_v2.csv\", output=\"../data/reduced_test.csv\", nrows=None, chunksize=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/reduced_train.csv\")\n",
    "test = pd.read_csv(\"../data/reduced_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date intervals to split the data\n",
    "x_train_dates=('2016-08-01', '2017-11-30') \n",
    "y_test_dates=('2017-12-01', '2018-01-31')\n",
    "x_test_dates=('2017-08-01', '2018-11-30')\n",
    "\n",
    "# Final data processing\n",
    "x_train, y_train, x_test = split_data(train, test, x_train_dates=x_train_dates, y_test_dates=y_test_dates, x_test_dates=x_test_dates, selec_top_per=0.5, max_cat=10)\n",
    "train, test = None, None \n",
    "\n",
    "# Save dfs as pickle objects -> faster to load and save. In addition, we do not need to worry about format issues\n",
    "x_train.to_pickle(\"../data/x_train.pkl\") \n",
    "y_train.to_pickle(\"../data/y_train.pkl\") \n",
    "x_test.to_pickle(\"../data/x_test.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Our first attempt!\n",
    "Let's see if we can fit a model on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that due to the one-hot encoding, the columns of train and test are not the same. For this experiment, only keep the intersection of columns. There are also other ways to deal with this (e.g., by mapping categories to external data), so we don't do this in the aggregation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_pickle(\"../data/x_train.pkl\") \n",
    "y_train = pd.read_pickle(\"../data/y_train.pkl\") \n",
    "x_test = pd.read_pickle(\"../data/x_test.pkl\") \n",
    "\n",
    "# Save the dataset ids\n",
    "id_x_train =  x_train['fullVisitorId']\n",
    "id_y_train = y_train['fullVisitorId']\n",
    "id_x_test = x_test['fullVisitorId']\n",
    "\n",
    "# Delete fullVisitor ID -> probably we want to leave it as a OHE feature\n",
    "del x_train['fullVisitorId']\n",
    "del y_train['fullVisitorId']\n",
    "del x_test['fullVisitorId']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NaNs to zero and fit linear model\n",
    "from sklearn import linear_model\n",
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)\n",
    "y_train = y_train.fillna(0)\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "r_squared = lm.score(x_train, y_train)\n",
    "print(\"The model has an R^2 of {}.\".format(r_squared))\n",
    "# do the prediction\n",
    "prediction = list(lm.predict(x_test).flat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
