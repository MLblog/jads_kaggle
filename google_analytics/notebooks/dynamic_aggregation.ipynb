{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Aggregation with dynamic features\n",
    "This notebook shows how to do the aggregation with monthly values for certain columns. \n",
    "\n",
    "__Remark:__ Because we now filter out a lot of users that only visited once, this notebook is not such a pain in the ass anymore. Don't be afraid to run it, your memory will be sufficient and you'll be done in a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "from preprocessing import *\n",
    "from aggregation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Managing a huge file\n",
    "\n",
    "Below is the new version of `load`, where processing takes place in chunks. After all chunks have been processed, they are concatenated to a single file. Since many columns are either dropped or aggregated, the resulting dataframe fits in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Only run these the first time - after it you can just load the reduced_datasets.\n",
    "reduce_df(\"../data/train_v2.csv\", output=\"../data/reduced_train.csv\", nrows=None, chunksize=20000)\n",
    "reduce_df(\"../data/test_v2.csv\", output=\"../data/reduced_test.csv\", nrows=None, chunksize=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Let's widen the train dataset.\")\n",
    "train = pd.read_csv(\"../data/reduced_train.csv\")\n",
    "wide_train = aggregate(train)\n",
    "wide_train.to_csv(\"../data/wide_train.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "print(\"Let's widen the test dataset.\")\n",
    "test = pd.read_csv(\"../data/reduced_test.csv\")\n",
    "wide_test = aggregate(test)\n",
    "wide_test.to_csv(\"../data/wide_test.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff\n",
    "\n",
    "The cells below include usage of aggregation functionality developed prior to the competition's restart. We keep it here in case we need to go back and check our initial ideas but these will typically not work equally well on the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load and preprocess the data\n",
    "First we need to load and preprocess the original data.\n",
    "Note that there might be three additional columns in the new dataset. These need to be preprocessed as well.\n",
    "\n",
    "The `preprocess_and_save` method now has an argument `drop_users=True` (default is True). You can set this to false if you wish to keep all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocess_and_save(\"../data\", nrows_train=None, nrows_test=None, start_x_train='2016-08-01', \n",
    "                    end_x_train='2016-10-16', start_y_train='2016-12-01', end_y_train='2017-02-01', \n",
    "                    start_x_test='2017-08-01', end_x_test='2017-10-16', drop_users=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Aggregating data per customer\n",
    "\n",
    "We need to predict the target at customer-level, i.e., we predict one value for each customer\n",
    "in the test set. Our data, however, contains a row for every site visit. One obvious way to deal\n",
    "with this discrepancy is to aggregate the visit data by customer, to obtain features on the\n",
    "customer-level.\n",
    "\n",
    "The _preprocessing.py_ file contains functions to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "x_train, y_train, x_test = load_train_test_dataframes(data_dir, nrows_train=None, nrows_test=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"This only takes a few seconds now.\")\n",
    "x_train_aggregated = aggregate_data_per_customer(x_train, startdate_y='2016-12-01', startdate_x='2016-08-01')\n",
    "print(\"Train data is aggregated\")\n",
    "print(\"Aggregating test data.\")\n",
    "x_test_aggregated = aggregate_data_per_customer(x_test, startdate_y='2017-12-01', startdate_x='2017-08-01')\n",
    "print(\"Test data is aggregated\")\n",
    "y_train_aggregated = y_train.groupby(['fullVisitorId'])[['target']].sum()\n",
    "print(\"Train target data is aggregated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This data should be close to what we need to start fitting models. We tried to keep as much information as possible, so from here it is of course still possible to remove features that seem unnecessary or do other dimensionality reduction. The good thing is that we don't have to do aggregation every time, so let's save the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Saving files\")\n",
    "x_train_aggregated.to_csv(os.path.join(data_dir, \"aggregated_x_train.csv\"), index=True)\n",
    "x_test_aggregated.to_csv(os.path.join(data_dir, \"aggregated_x_test.csv\"), index=True)\n",
    "y_train_aggregated.to_csv(os.path.join(data_dir, \"aggregated_y_train.csv\"), index=True)\n",
    "print(\"Aggregated data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Our first attempt!\n",
    "Let's see if we can fit a model on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that due to the one-hot encoding, the columns of train and test are not the same. For this experiment, only keep the intersection of columns. There are also other ways to deal with this (e.g., by mapping categories to external data), so we don't do this in the aggregation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# just for illustration, so let's keep it simple\n",
    "from sklearn import linear_model\n",
    "\n",
    "# create train and test sets and labels excluding visitor ID\n",
    "x_train, x_test = keep_intersection_of_columns(aggregated_train.reset_index(drop=True),\n",
    "                                               aggregated_test.reset_index(drop=True))\n",
    "y_train = np.log(aggregated_train.reset_index(drop=True)[\"target_sum\"]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set NaNs to zero and fit linear model\n",
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "r_squared = lm.score(x_train, y_train)\n",
    "print(\"The model has an R^2 of {}.\".format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# predict and create a submission\n",
    "predictions = lm.predict(x_test)\n",
    "submission = pd.concat([aggregated_test.reset_index()[\"fullVisitorId\"], pd.Series(predictions)], axis=1)\n",
    "submission.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n",
    "\n",
    "# set everything below $1 to zero\n",
    "submission[\"PredictedLogRevenue\"] = np.maximum(0, submission[\"PredictedLogRevenue\"])\n",
    "submission[\"PredictedLogRevenue\"][submission[\"PredictedLogRevenue\"]<1] = 0\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"first_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
