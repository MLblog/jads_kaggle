{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation with dynamic features\n",
    "This notebook shows how to do the aggregation with monthly values for certain columns. \n",
    "\n",
    "__Remark:__ Because we now filter out a lot of users that only visited once, this notebook is not such a pain in the ass anymore. Don't be afraid to run it, your memory will be sufficient and you'll be done in a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "from preprocessing import *\n",
    "from aggregation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing a huge file\n",
    "\n",
    "Below is the new version of `load`, where processing takes place in chunks. After all chunks have been processed, they are concatenated to a single file. Since many columns are either dropped or aggregated, the resulting dataframe fits in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunk 0, |Shape is: (20000, 9)\n",
      "Loaded chunk 1, |Shape is: (20000, 9)\n",
      "Loaded chunk 2, |Shape is: (20000, 9)\n",
      "Loaded chunk 3, |Shape is: (20000, 9)\n",
      "Loaded chunk 4, |Shape is: (20000, 9)\n",
      "Loaded chunk 5, |Shape is: (20000, 9)\n",
      "Loaded chunk 6, |Shape is: (20000, 9)\n",
      "Loaded chunk 7, |Shape is: (20000, 9)\n",
      "Loaded chunk 8, |Shape is: (20000, 9)\n",
      "Loaded chunk 9, |Shape is: (20000, 9)\n",
      "Loaded chunk 10, |Shape is: (20000, 9)\n",
      "Loaded chunk 11, |Shape is: (20000, 9)\n",
      "Loaded chunk 12, |Shape is: (20000, 9)\n",
      "Loaded chunk 13, |Shape is: (20000, 9)\n",
      "Loaded chunk 14, |Shape is: (20000, 9)\n",
      "Loaded chunk 15, |Shape is: (20000, 9)\n",
      "Loaded chunk 16, |Shape is: (20000, 9)\n",
      "Loaded chunk 17, |Shape is: (20000, 9)\n",
      "Loaded chunk 18, |Shape is: (20000, 9)\n",
      "Loaded chunk 19, |Shape is: (20000, 9)\n",
      "Loaded chunk 20, |Shape is: (20000, 9)\n",
      "Loaded chunk 21, |Shape is: (20000, 9)\n",
      "Loaded chunk 22, |Shape is: (20000, 9)\n",
      "Loaded chunk 23, |Shape is: (20000, 9)\n",
      "Loaded chunk 24, |Shape is: (20000, 9)\n",
      "Loaded chunk 25, |Shape is: (20000, 9)\n",
      "Loaded chunk 26, |Shape is: (20000, 9)\n",
      "Loaded chunk 27, |Shape is: (20000, 9)\n",
      "Loaded chunk 28, |Shape is: (20000, 9)\n",
      "Loaded chunk 29, |Shape is: (20000, 9)\n",
      "Loaded chunk 30, |Shape is: (20000, 9)\n",
      "Loaded chunk 31, |Shape is: (20000, 9)\n",
      "Loaded chunk 32, |Shape is: (20000, 9)\n",
      "Loaded chunk 33, |Shape is: (20000, 9)\n",
      "Loaded chunk 34, |Shape is: (20000, 9)\n",
      "Loaded chunk 35, |Shape is: (20000, 9)\n",
      "Loaded chunk 36, |Shape is: (20000, 9)\n",
      "Loaded chunk 37, |Shape is: (20000, 9)\n",
      "Loaded chunk 38, |Shape is: (20000, 9)\n",
      "Loaded chunk 39, |Shape is: (20000, 9)\n",
      "Loaded chunk 40, |Shape is: (20000, 9)\n",
      "Loaded chunk 41, |Shape is: (20000, 9)\n",
      "Loaded chunk 42, |Shape is: (20000, 9)\n",
      "Loaded chunk 43, |Shape is: (20000, 9)\n",
      "Loaded chunk 44, |Shape is: (20000, 9)\n",
      "Loaded chunk 45, |Shape is: (20000, 9)\n",
      "Loaded chunk 46, |Shape is: (20000, 9)\n",
      "Loaded chunk 47, |Shape is: (20000, 9)\n",
      "Loaded chunk 48, |Shape is: (20000, 9)\n",
      "Loaded chunk 49, |Shape is: (20000, 9)\n",
      "Loaded chunk 50, |Shape is: (20000, 9)\n",
      "Loaded chunk 51, |Shape is: (20000, 9)\n",
      "Loaded chunk 52, |Shape is: (20000, 9)\n",
      "Loaded chunk 53, |Shape is: (20000, 9)\n",
      "Loaded chunk 54, |Shape is: (20000, 9)\n",
      "Loaded chunk 55, |Shape is: (20000, 9)\n",
      "Loaded chunk 56, |Shape is: (20000, 9)\n",
      "Loaded chunk 57, |Shape is: (20000, 9)\n",
      "Loaded chunk 58, |Shape is: (20000, 9)\n",
      "Loaded chunk 59, |Shape is: (20000, 9)\n",
      "Loaded chunk 60, |Shape is: (20000, 9)\n",
      "Loaded chunk 61, |Shape is: (20000, 9)\n",
      "Loaded chunk 62, |Shape is: (20000, 9)\n",
      "Loaded chunk 63, |Shape is: (20000, 9)\n",
      "Loaded chunk 64, |Shape is: (20000, 9)\n",
      "Loaded chunk 65, |Shape is: (20000, 9)\n",
      "Loaded chunk 66, |Shape is: (20000, 9)\n",
      "Loaded chunk 67, |Shape is: (20000, 9)\n",
      "Loaded chunk 68, |Shape is: (20000, 9)\n",
      "Loaded chunk 69, |Shape is: (20000, 9)\n",
      "Loaded chunk 70, |Shape is: (20000, 9)\n",
      "Loaded chunk 71, |Shape is: (20000, 9)\n",
      "Loaded chunk 72, |Shape is: (20000, 9)\n",
      "Loaded chunk 73, |Shape is: (20000, 9)\n",
      "Loaded chunk 74, |Shape is: (20000, 9)\n",
      "Loaded chunk 75, |Shape is: (20000, 9)\n",
      "Loaded chunk 76, |Shape is: (20000, 9)\n",
      "Loaded chunk 77, |Shape is: (20000, 9)\n",
      "Loaded chunk 78, |Shape is: (20000, 9)\n",
      "Loaded chunk 79, |Shape is: (20000, 9)\n",
      "Loaded chunk 80, |Shape is: (20000, 9)\n",
      "Loaded chunk 81, |Shape is: (20000, 9)\n",
      "Loaded chunk 82, |Shape is: (20000, 9)\n",
      "Loaded chunk 83, |Shape is: (20000, 9)\n",
      "Loaded chunk 84, |Shape is: (20000, 9)\n",
      "Loaded chunk 85, |Shape is: (8337, 9)\n",
      "Finished all chunks, now concatenating\n"
     ]
    }
   ],
   "source": [
    "def reduce_df(path, output, nrows=None, chunksize=20000):\n",
    "    \"\"\" Load Google analytics data from JSON into a Pandas.DataFrame. \"\"\"\n",
    "    if nrows and chunksize:\n",
    "        msg = \"Reading {} rows in chunks of {}. We are gonna need {} chunks\"\n",
    "        print(msg.format(nrows, chunksize, nrows / chunksize))\n",
    "    \n",
    "    temp_dir = \"../data/temp\"\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "    \n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    i = 0\n",
    "    for chunk in pd.read_csv(path, \n",
    "                             converters={column: json.loads for column in JSON_COLUMNS},\n",
    "                             dtype={'fullVisitorId': 'str'},\n",
    "                             nrows=nrows,\n",
    "                             chunksize=chunksize):\n",
    "        \n",
    "        chunk = chunk.reset_index()\n",
    "        \n",
    "        # Normalize JSON columns\n",
    "        for column in JSON_COLUMNS:\n",
    "            column_as_df = pd.io.json.json_normalize(chunk[column])\n",
    "            chunk = chunk.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "        # Parse date\n",
    "        chunk['date'] = chunk['date'].apply(lambda x: pd.datetime.strptime(str(x), '%Y%m%d'))\n",
    "        \n",
    "        # Only keep relevant columns\n",
    "        cols = ['date', 'fullVisitorId', 'operatingSystem', 'country', 'browser',\n",
    "                'pageviews', 'transactions', 'visits', 'transactionRevenue']\n",
    "        try:\n",
    "            chunk = chunk[cols]\n",
    "        except KeyError as e:\n",
    "            # Regex magic to find exactly which columns were not found.\n",
    "            # Might be different in Python 3, be careful!\n",
    "            missing_cols = list(re.findall(r\"'(.*?)'\", e.args[0]))\n",
    "            for col in missing_cols:\n",
    "                print(\"Column {} was not found in chunk {}, filling with zeroes\".format(col, i))\n",
    "                chunk[col] = [0] * len(chunk)\n",
    "            chunk = chunk[cols]\n",
    "            \n",
    "        \n",
    "        print(\"Loaded chunk {}, Shape is: {}\".format(i, chunk.shape))\n",
    "        chunk.to_csv(os.path.join(temp_dir, str(i) + \".csv\"), encoding='utf-8', index=False)\n",
    "        i += 1\n",
    "    \n",
    "    print(\"Finished all chunks, now concatenating\")\n",
    "    files = glob.glob(os.path.join(temp_dir, \"*.csv\"))\n",
    "    with open(output, 'wb') as outfile:\n",
    "        for i, fname in enumerate(files):\n",
    "            with open(fname, 'rb') as infile:\n",
    "                # Throw away header on all but first file\n",
    "                if i != 0:\n",
    "                    infile.readline()  \n",
    "                # Block copy rest of file from input to output without parsing\n",
    "                shutil.copyfileobj(infile, outfile)\n",
    "\n",
    "reduce_df(\"../data/train_v2.csv\", output=\"../data/reduced_train.csv\", nrows=None, chunksize=20000)\n",
    "reduce_df(\"../data/test_v2.csv\", output=\"../data/reduced_test.csv\", nrows=None, chunksize=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunk 0, |Shape is: (20000, 9)\n",
      "Loaded chunk 1, |Shape is: (20000, 9)\n",
      "Loaded chunk 2, |Shape is: (20000, 9)\n",
      "Loaded chunk 3, |Shape is: (20000, 9)\n",
      "Loaded chunk 4, |Shape is: (20000, 9)\n",
      "Loaded chunk 5, |Shape is: (20000, 9)\n",
      "Loaded chunk 6, |Shape is: (20000, 9)\n",
      "Loaded chunk 7, |Shape is: (20000, 9)\n",
      "Loaded chunk 8, |Shape is: (20000, 9)\n",
      "Loaded chunk 9, |Shape is: (20000, 9)\n",
      "Loaded chunk 10, |Shape is: (20000, 9)\n",
      "Loaded chunk 11, |Shape is: (20000, 9)\n",
      "Loaded chunk 12, |Shape is: (20000, 9)\n",
      "Loaded chunk 13, |Shape is: (20000, 9)\n",
      "Loaded chunk 14, |Shape is: (20000, 9)\n",
      "Loaded chunk 15, |Shape is: (20000, 9)\n",
      "Loaded chunk 16, |Shape is: (20000, 9)\n",
      "Loaded chunk 17, |Shape is: (20000, 9)\n",
      "Loaded chunk 18, |Shape is: (20000, 9)\n",
      "Loaded chunk 19, |Shape is: (20000, 9)\n",
      "Loaded chunk 20, |Shape is: (1589, 9)\n",
      "Finished all chunks, now concatenating\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/reduced_train.csv\")\n",
    "train['transactions'].fillna(0, inplace=True)\n",
    "train['transactionRevenue'].fillna(0.0, inplace=True)\n",
    "\n",
    "def agg_date(s):\n",
    "    date = pd.datetime.strptime(x, '%Y-%m-%d')\n",
    "    return \"{}_{}\".format(x.month, x.year)\n",
    "        \n",
    "train[\"date\"] = train[\"date\"].apply(agg_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent = lambda x:x.value_counts().index[0]\n",
    "agg = {\n",
    "    \"operatingSystem\": most_frequent,\n",
    "    \"country\": most_frequent,\n",
    "    \"browser\": most_frequent,\n",
    "    \"pageviews\": sum,\n",
    "    \"transactions\": sum,\n",
    "    \"visits\": sum,\n",
    "    \"transactionRevenue\": sum\n",
    "}\n",
    "grouped = train.groupby([\"fullVisitorId\", \"date\"], as_index=False).agg(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.to_csv(\"../data/grouped.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the data\n",
    "First we need to load and preprocess the original data.\n",
    "Note that there might be three additional columns in the new dataset. These need to be preprocessed as well.\n",
    "\n",
    "The `preprocess_and_save` method now has an argument `drop_users=True` (default is True). You can set this to false if you wish to keep all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save(\"../data\", nrows_train=None, nrows_test=None, start_x_train='2016-08-01', \n",
    "                    end_x_train='2016-10-16', start_y_train='2016-12-01', end_y_train='2017-02-01', \n",
    "                    start_x_test='2017-08-01', end_x_test='2017-10-16', drop_users=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating data per customer\n",
    "\n",
    "We need to predict the target at customer-level, i.e., we predict one value for each customer\n",
    "in the test set. Our data, however, contains a row for every site visit. One obvious way to deal\n",
    "with this discrepancy is to aggregate the visit data by customer, to obtain features on the\n",
    "customer-level.\n",
    "\n",
    "The _preprocessing.py_ file contains functions to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "x_train, y_train, x_test = load_train_test_dataframes(data_dir, nrows_train=None, nrows_test=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This only takes a few seconds now.\")\n",
    "x_train_aggregated = aggregate_data_per_customer(x_train, startdate_y='2016-12-01', startdate_x='2016-08-01')\n",
    "print(\"Train data is aggregated\")\n",
    "print(\"Aggregating test data.\")\n",
    "x_test_aggregated = aggregate_data_per_customer(x_test, startdate_y='2017-12-01', startdate_x='2017-08-01')\n",
    "print(\"Test data is aggregated\")\n",
    "y_train_aggregated = y_train.groupby(['fullVisitorId'])[['target']].sum()\n",
    "print(\"Train target data is aggregated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data should be close to what we need to start fitting models. We tried to keep as much information as possible, so from here it is of course still possible to remove features that seem unnecessary or do other dimensionality reduction. The good thing is that we don't have to do aggregation every time, so let's save the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving files\")\n",
    "x_train_aggregated.to_csv(os.path.join(data_dir, \"aggregated_x_train.csv\"), index=True)\n",
    "x_test_aggregated.to_csv(os.path.join(data_dir, \"aggregated_x_test.csv\"), index=True)\n",
    "y_train_aggregated.to_csv(os.path.join(data_dir, \"aggregated_y_train.csv\"), index=True)\n",
    "print(\"Aggregated data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first attempt!\n",
    "Let's see if we can fit a model on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the one-hot encoding, the columns of train and test are not the same. For this experiment, only keep the intersection of columns. There are also other ways to deal with this (e.g., by mapping categories to external data), so we don't do this in the aggregation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for illustration, so let's keep it simple\n",
    "from sklearn import linear_model\n",
    "\n",
    "# create train and test sets and labels excluding visitor ID\n",
    "x_train, x_test = keep_intersection_of_columns(aggregated_train.reset_index(drop=True),\n",
    "                                               aggregated_test.reset_index(drop=True))\n",
    "y_train = np.log(aggregated_train.reset_index(drop=True)[\"target_sum\"]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NaNs to zero and fit linear model\n",
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "r_squared = lm.score(x_train, y_train)\n",
    "print(\"The model has an R^2 of {}.\".format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and create a submission\n",
    "predictions = lm.predict(x_test)\n",
    "submission = pd.concat([aggregated_test.reset_index()[\"fullVisitorId\"], pd.Series(predictions)], axis=1)\n",
    "submission.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n",
    "\n",
    "# set everything below $1 to zero\n",
    "submission[\"PredictedLogRevenue\"] = np.maximum(0, submission[\"PredictedLogRevenue\"])\n",
    "submission[\"PredictedLogRevenue\"][submission[\"PredictedLogRevenue\"]<1] = 0\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"first_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
