{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Earthquake Prediction\n",
    "\n",
    "This notebook shows some new functionality that will be useful for applying deep learning on the earthquake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, Flatten, Dropout, LSTM\n",
    "from keras import backend as K\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from common.utils import progress\n",
    "from earthquakes.engineering import save_earthquake_cycles, get_cycle\n",
    "from earthquakes.deep import Scaler, KFoldCycles, train_on_cycles, evaluate_on_cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the earthquake data by cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"../data\"\n",
    "data_dir = \"D:/KaggleData/earthquakes/\"\n",
    "\n",
    "# train = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "train = pickle.load(open(os.path.join(data_dir, \"train.pickle\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `engineering` module now has two additional functions:\n",
    "1. `find_earthquakes`: calculates the exact timing of earthquakes in a few chunks (3 by default) to prevent memory issues, while maintaining speed.\n",
    "2. `save_earthquake_cycles`: calls `find_earthquakes` and then saves the entire training data per cycle. Note that there will be 17 cycles in total, since there is still data after the last earthquake that we can use.\n",
    "\n",
    "The whole thing runs in a about 30 seconds, so no worries here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_earthquake_cycles(train, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the cycles, we can delete the training data from memory, as we won't need it anymore. Instead, we will train on the cycles, one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper classes\n",
    "\n",
    "There are two helper classes in the `deep` module. The first is the `Scaler`, which implements various scaling methods (see docstring for all of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a Scaler with a custom scaling value\n",
    "scaler = Scaler(method=\"value\", value=300)\n",
    "# example: \n",
    "scaler.scale([150, 4.5, 2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is `KFoldCycles` which is a splitter like `sklearn`'s `KFold`, but instead splits cycle numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = KFoldCycles()\n",
    "for train_cycles, val_cycles in splitter.split():\n",
    "    print(\"train: {}, val: {}\".format(train_cycles, val_cycles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Deep Learning model\n",
    "\n",
    "Let's first define a keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(32, kernel_size=15, strides=8, padding=\"causal\", activation=\"relu\", input_shape=(150000, 1)))\n",
    "model.add(Conv1D(32, kernel_size=15, strides=3, padding=\"causal\", activation=\"relu\"))\n",
    "model.add(Conv1D(32, kernel_size=15, strides=3, padding=\"causal\", activation=\"relu\"))\n",
    "model.add(Conv1D(32, kernel_size=15, strides=3, padding=\"causal\", activation=\"relu\"))\n",
    "# model.add(LSTM(16, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `deep` module has two more functions that make it very easy to train and evaluate a model on different cycles. They are simply called `train_on_cycles` and `evaluate_on_cycles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_losses = []\n",
    "for train_cycles, val_cycles in splitter.split():\n",
    "    model = train_on_cycles(model, epochs=4, cycle_nrs=train_cycles, scaler=scaler, data_dir=data_dir)\n",
    "    loss, cycle_losses, cycle_weights = evaluate_on_cycles(model, cycle_nrs=val_cycles, scaler=scaler, data_dir=data_dir)\n",
    "    cv_losses.append(loss)\n",
    "\n",
    "print(\"Mean Cross-Validation loss: {}\".format(np.mean(cv_losses)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
