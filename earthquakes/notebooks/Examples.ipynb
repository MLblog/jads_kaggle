{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of current code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from earthquakes.engineering import sequence_generator, FeatureComputer, create_feature_dataset\n",
    "from earthquakes.modeling import train_and_predict, cv_with_feature_computer, predict_on_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.precision = 15\n",
    "data_dir = \"../data\"\n",
    "\n",
    "train = pd.read_csv(os.path.join(data_dir, \"train.csv\"),\n",
    "                    dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate the work in the starter notebook with the functions from the `engineering` and `modeling` modules. Let's use a slightly different model though and some more quantiles.\n",
    "\n",
    "In addition, the cross validation method now has an option to predict on the test set at every fold by setting `predict_on_test=True`. In that case, the method returns a dataframe with predictions on the test set besides the cross validation scores. We can use this to blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computer = FeatureComputer(quantiles=[0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99])\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"loss\": 'lad',\n",
    "    \"verbose\": 1,\n",
    "}\n",
    "\n",
    "scores, test_predictions = cv_with_feature_computer(train, GradientBoostingRegressor, computer,\n",
    "                                                    train_samples=5000, val_samples=1000,\n",
    "                                                    predict_test=True, data_dir=data_dir)\n",
    "\n",
    "print(\"Cross validation score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try blending by averaging over the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_predictions[[\"seg_id\", \"time_to_failure\"]].copy()\n",
    "submission[\"time_to_failure\"] = test_predictions.drop(\"seg_id\", axis=1).mean(axis=1)\n",
    "submission.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(data_dir, \"submissions\", \"gradient_boosting_with_blending.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This notebook achieved 1.592 on the public leaderboard (again, without any tuning whatsoever).__ I just added simple blending, used Gradient Boosting instead of Random Forest, and used some more training data in every fold (5000 samples vs 1000)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
