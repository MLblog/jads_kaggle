{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of current code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from earthquakes.engineering import sequence_generator, FeatureComputer, create_feature_dataset\n",
    "from earthquakes.modeling import train_and_predict, cv_with_feature_computer, predict_on_test, create_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.precision = 15\n",
    "data_dir = \"../data\"\n",
    "\n",
    "train = pd.read_csv(os.path.join(data_dir, \"train.csv\"),\n",
    "                    dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float64})\n",
    "\n",
    "# # save as pickle for fasting loading\n",
    "# train.to_pickle(os.path.join(data_dir, \"train.pickle\"))\n",
    "# # and load it\n",
    "# train = pickle.load(open(os.path.join(data_dir, \"train.pickle\"), \"rb\"))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate the work in the starter notebook with the functions from the `engineering` and `modeling` modules. Let's use a slightly different model though and some more quantiles.\n",
    "\n",
    "__Added functionality:__\n",
    "- The cross validation method now has an option to predict on the test set at every fold by setting `predict_on_test=True`. In that case, the method returns a dataframe with predictions on the test set besides the cross validation scores. We can use this to blending.\n",
    "- There is the option to `Compute the Short Time Fourier Transform` by setting `stft=True`. From this transformation, the same statistics are calculated as for the usual time interval giving additional information from the signal. Note that there are some parameters of this transformation that we can play with. For further information, please see [the docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.stft.html).\n",
    "- The available features in FeatureComputer is extended and features can now be calculated over windows within a sequence (subsequences) by setting, e.g., `window=5000` in the FeatureComputer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission using the new features (including stft), calculated over 10 subsequences (windows) per sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99]\n",
    "big = [100, 200, 500, 1000]\n",
    "stalta = [(50, 1000), (100, 1500), (500, 5000), (1000, 10000), (5000, 15000), (10000, 25000)]\n",
    "stalta_window = [(50, 1000), (100, 1500), (500, 5000), (1000, 5000)]\n",
    "exp_mov_ave = [300, 3000, 10000]\n",
    "exp_mov_ave_window = [300, 1000, 2000]\n",
    "\n",
    "computer = FeatureComputer(quantiles=q, abs_quantiles=q, count_abs_big=big, stalta=stalta, stalta_window=stalta_window,\n",
    "                           exp_mov_ave=exp_mov_ave, exp_mov_ave_window=exp_mov_ave_window, window=15000)\n",
    "stft_computer = FeatureComputer(quantiles=q, abs_quantiles=q, count_abs_big=big) # no windows, STALTA, and exp_mov_ave for stft\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"loss\": 'lad',\n",
    "    \"verbose\": 1,\n",
    "}\n",
    "\n",
    "scores, test_predictions = cv_with_feature_computer(train, GradientBoostingRegressor, computer,\n",
    "                                                    train_samples=500, val_samples=100,\n",
    "                                                    predict_test=True, data_dir=data_dir,\n",
    "                                                    stft=True, stft_feature_computer=stft_computer)\n",
    "\n",
    "print(\"Cross validation score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try blending by averaging over the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_predictions[[\"seg_id\", \"time_to_failure\"]].copy()\n",
    "submission[\"time_to_failure\"] = test_predictions.drop(\"seg_id\", axis=1).mean(axis=1)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(data_dir, \"submissions\", \"submission.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Leaderboard scores:__\n",
    "- First version with RF without blending: __1.758__\n",
    "- Still with basic features (minimum, maximum, mean, median, std, quantiles), GradientBoostingRegressor, and blending: __1.592__\n",
    "- Using `stft=True` and XGBoost: __1.546__.\n",
    "- Using new features (PR 128), stft, and GradientBoostingRegressor: __1.544__.\n",
    "- Using new features (PR 128 and 130), stft, and GradientBoostingRegressor: __1.533__.\n",
    "\n",
    "\n",
    "### Create a test dataset\n",
    "We can now create a test dataset, so that we don't compute features on the same data in every fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = create_test_dataset(computer, data_dir=data_dir, stft=True, stft_feature_computer=stft_computer)\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use it in cross validation like so:\n",
    "\n",
    "```\n",
    ">>> scores, test_predictions = cv_with_feature_computer(..., test_data=x_test, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, test_predictions = cv_with_feature_computer(train, GradientBoostingRegressor, computer,\n",
    "                                                    test_data=x_test, train_samples=500, val_samples=100,\n",
    "                                                    predict_test=True, data_dir=data_dir,\n",
    "                                                    stft=True, stft_feature_computer=stft_computer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
