{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Data From Hard-Disck Memory\n",
    "Here some functions that uses data from memory to create the ML model\n",
    "\n",
    "#### TODO:\n",
    "- try parallel computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data Manipulation\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.precision = 15\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "#  Machine learning\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Other functions\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  Our functions\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from earthquakes.helpers import Caller, create_feature_dataset_source\n",
    "from earthquakes.engineering import FeatureComputer\n",
    "from earthquakes.modeling import predict_on_test\n",
    "from common.utils import save_object, load_object\n",
    "\n",
    "#  Data directories\n",
    "data_dir = \"../data\"\n",
    "save_dir = \"../data/chunk_signal\"\n",
    "save_class = \"../data/classes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up the data\n",
    "First, the data must be stored in memory in chunks. In order to do so, __we have to run this block of code only ones__. Unless you want to change parameters such as size. In this case, you have to run it again. Be aware that you have to delete the files on the destiny folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(os.path.join(data_dir, \"train.csv\"),\n",
    "#                     dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float64})\n",
    "train = pd.read_pickle(os.path.join(data_dir, \"train.pickle\"))\n",
    "caller_cls = Caller(save_dir=save_dir, size=150000)\n",
    "caller_cls.save_data(train)\n",
    "save_object(save_path=os.path.join(save_class, \"caller_cls.pkl\"), object_ = caller_cls)\n",
    "train = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lest get the events\n",
    "Here a very simple implementation to illustrate how this method can be used getting the index values when there is an earthquake. Before we were having memory errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caller_cls = load_object(os.path.join(save_class, \"caller_cls.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ilustrate the most basic funtionality of this class, here you can set any initial id and window size. You will get back only the information that you require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caller_cls.get_intervals(i_init=60, window_size=150000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over all the signal to get the events (earthquakes). This is done in only two minutes! Which is remarkable considering the size of the dataset. The events positions are:\n",
    "- [5656573,\n",
    " 50085877,\n",
    " 104677355,\n",
    " 138772452,\n",
    " 187641819,\n",
    " 218652629,\n",
    " 245829584,\n",
    " 307838916,\n",
    " 338276286,\n",
    " 375377847,\n",
    " 419368879,\n",
    " 461811622,\n",
    " 495800224,\n",
    " 528777114,\n",
    " 585568143,\n",
    " 621985672]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events():\n",
    "    \"\"\"Funtion to get events (earthquakes). \n",
    "    Because there is not 0 time, an event is defined\n",
    "    when the difference between two observatiosn in time\n",
    "    is positive.\n",
    "    \"\"\"\n",
    "    events = []\n",
    "    for idx in tqdm(caller_cls.index_list[:-1]):\n",
    "        try:\n",
    "            delta = caller_cls.get_intervals(i_init = idx, window_size=150000)['time_to_failure'].diff()\n",
    "        except:\n",
    "            print(idx)\n",
    "        delta = delta[delta > 0]\n",
    "        events.append(list(delta.index -1))\n",
    "        delta = None\n",
    "    return [x[0] for x in events if x]\n",
    "\n",
    "events_id = get_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Geting the data per cycle (information between events):\n",
    "It makes sense to train and test our models on different cycles. In order to do so, here we create the datasets for every cycle independently. I skip the cycle that starts from 0 because there is no guarantee that it is a complete cycle. Same issue with the last interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caller_cls = load_object(os.path.join(save_class, \"caller_cls.pkl\"))\n",
    "q = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99]\n",
    "computer = FeatureComputer(quantiles=q, abs_quantiles=q)\n",
    "events = [5656573, 50085877, 104677355, 138772452, 187641819, 218652629, 245829584,\n",
    "          307838916, 338276286, 375377847, 419368879, 461811622, 495800224, 528777114,\n",
    "          585568143, 621985672]\n",
    "tuples = [(x, y - 150000) for x, y in zip(events, events[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_0, i_n in tuples:\n",
    "    name = str(i_0) + '_' + str(i_n)\n",
    "    new_data = create_feature_dataset_source(caller_cls,\n",
    "                                             feature_computer=computer,\n",
    "                                             step=5000,\n",
    "                                             stft=True, \n",
    "                                             events_id=(i_0, i_n))\n",
    "    new_data.to_pickle(os.path.join(data_dir, \"cycle_s5000_{}.pkl\".format(name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Some experiments for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_pickle(os.path.join(data_dir, \"cycle_s5000_5656573_49935877.pkl\"))\n",
    "data_val = pd.read_pickle(os.path.join(data_dir, \"cycle_s5000_138772452_187491819.pkl\"))\n",
    "X_train, X_valid, y_train, y_valid  = data_train, data_val, data_train[\"time_to_failure\"], data_val[\"time_to_failure\"]\n",
    "del X_train[\"time_to_failure\"]\n",
    "del X_valid[\"time_to_failure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(X_train[\"maximum\"].values[-2000:]).plot()\n",
    "pd.Series(X_valid[\"maximum\"].values[-2000:]).plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 54,\n",
    "          'min_data_in_leaf': 79,\n",
    "          'objective': 'huber',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8126672064208567,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1302650970728192,\n",
    "          'reg_lambda': 0.3603427518866501\n",
    "         }\n",
    "\n",
    "model = XGBRegressor(num_boost_round=20000,\n",
    "                     early_stopping_rounds=200,\n",
    "                     verbose_eval=500,\n",
    "                     params=params,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "# model = LinearRegression() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "scores = -1 * cross_val_score(model, X_valid, y_valid, cv=5, scoring='neg_mean_absolute_error')\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99]\n",
    "computer = FeatureComputer(quantiles=q, abs_quantiles=q)\n",
    "submission = predict_on_test(model=model,\n",
    "                             feature_computer=computer,\n",
    "                             stft_feature_computer=computer,\n",
    "                             ycol=\"time_to_failure\",\n",
    "                             stft=True,\n",
    "                             data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(data_dir, \"submissions\", \"submission_full_cycle.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
