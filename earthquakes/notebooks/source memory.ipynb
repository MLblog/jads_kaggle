{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Data From Hard-Disck Memory\n",
    "Here some functions that uses data from memory to create the ML model\n",
    "\n",
    "#### TODO:\n",
    "- try parallel computing (boost computational time to create datasets)\n",
    "- try LightGBM (good for sparse matrixes)\n",
    "- try CatBoost (avoid overfitting)\n",
    "- try Bayesian optimization for parametrization tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data Manipulation\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.precision = 15\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "#  Machine learning\n",
    "# from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Other libraries\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#  Our functions\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from earthquakes.helpers import (Caller,\n",
    "                                 create_feature_dataset_source,\n",
    "                                 create_signal_dataset,\n",
    "                                 check_distributions_train_test)\n",
    "\n",
    "from earthquakes.engineering import FeatureComputer\n",
    "from earthquakes.modeling import predict_on_test\n",
    "from common.utils import save_object, load_object\n",
    "\n",
    "#  Data directories\n",
    "data_dir = \"../data\"\n",
    "save_dir = \"../data/chunk_signal\"\n",
    "save_class = \"../data/classes\"\n",
    "data_cycle = \"../data/data cycles\"\n",
    "data_dp = \"../data/data deep learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up the data\n",
    "First, the data must be stored in memory in chunks. In order to do so, __we have to run this block of code only ones__. Unless you want to change parameters such as size. In this case, you have to run it again. Be aware that you have to delete the files on the destiny folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(os.path.join(data_dir, \"train.csv\"),\n",
    "#                     dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float64})\n",
    "train = pd.read_pickle(os.path.join(data_dir, \"train.pickle\"))\n",
    "caller_cls = Caller(save_dir=save_dir, size=150000)\n",
    "caller_cls.save_data(train)\n",
    "save_object(save_path=os.path.join(save_class, \"caller_cls.pkl\"), object_ = caller_cls)\n",
    "train = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lest get the events\n",
    "Here a very simple implementation to illustrate how this method can be used getting the index values when there is an earthquake. Before we were having memory errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caller_cls = load_object(os.path.join(save_class, \"caller_cls.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ilustrate the most basic funtionality of this class, here you can set any initial id and window size. You will get back only the information that you require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caller_cls.get_intervals(i_init=60, window_size=150000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over all the signal to get the events (earthquakes). This is done in only two minutes! Which is remarkable considering the size of the dataset. The events positions are:\n",
    "- [5656573,\n",
    " 50085877,\n",
    " 104677355,\n",
    " 138772452,\n",
    " 187641819,\n",
    " 218652629,\n",
    " 245829584,\n",
    " 307838916,\n",
    " 338276286,\n",
    " 375377847,\n",
    " 419368879,\n",
    " 461811622,\n",
    " 495800224,\n",
    " 528777114,\n",
    " 585568143,\n",
    " 621985672]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events():\n",
    "    \"\"\"Funtion to get events (earthquakes). \n",
    "    Because there is not 0 time, an event is defined\n",
    "    when the difference between two observatiosn in time\n",
    "    is positive.\n",
    "    \"\"\"\n",
    "    events = []\n",
    "    for idx in tqdm(caller_cls.index_list[:-1]):\n",
    "        try:\n",
    "            delta = caller_cls.get_intervals(i_init = idx, window_size=150000)['time_to_failure'].diff()\n",
    "        except:\n",
    "            print(idx)\n",
    "        delta = delta[delta > 0]\n",
    "        events.append(list(delta.index -1))\n",
    "        delta = None\n",
    "    return [x[0] for x in events if x]\n",
    "\n",
    "events_id = get_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Geting the data per cycle (information between events):\n",
    "It makes sense to train and test our models on different cycles. In order to do so, here we create the datasets for every cycle independently. I skip the cycle that starts from 0 because there is no guarantee that it is a complete cycle. Same issue with the last interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caller_cls = load_object(os.path.join(save_class, \"caller_cls.pkl\"))\n",
    "q = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99]\n",
    "big = [100, 200, 500, 1000]\n",
    "stalta = [(50, 1000), (100, 1500), (500, 5000), (1000, 10000), (5000, 15000), (10000, 25000)]\n",
    "stalta_window = [(50, 1000), (100, 1500), (500, 5000), (1000, 5000)]\n",
    "exp_mov_ave = [300, 3000, 10000]\n",
    "exp_mov_ave_window = [300, 1000, 2000]\n",
    "\n",
    "computer = FeatureComputer(quantiles=q, abs_quantiles=q, count_abs_big=big, stalta=stalta, stalta_window=stalta_window,\n",
    "                           exp_mov_ave=exp_mov_ave, exp_mov_ave_window=exp_mov_ave_window, window=15000)\n",
    "stft_computer = FeatureComputer(quantiles=q, abs_quantiles=q, count_abs_big=big) # no windows, STALTA, and exp_mov_ave for stft\n",
    "\n",
    "events_id = [0, 5656573, 50085877, 104677355, 138772452, 187641819, 218652629, 245829584,\n",
    "          307838916, 338276286, 375377847, 419368879, 461811622, 495800224, 528777114,\n",
    "          585568143, 621985672]\n",
    "tuples = [(x, y - 150000) for x, y in zip(events_id, events_id[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_0, i_n in tuples:\n",
    "    name = str(i_0) + '_' + str(i_n)\n",
    "    new_data = None\n",
    "    new_data = create_feature_dataset_source(caller_cl=caller_cls,\n",
    "                                             feature_computer=computer,\n",
    "                                             step=4096,\n",
    "                                             stft_feature_computer=stft_computer,\n",
    "                                             stft=True,\n",
    "                                             window_size=150000,\n",
    "                                             events_id=(i_0, i_n))\n",
    "    new_data.to_pickle(os.path.join(data_cycle, \"cycle_s5000_{}.pkl\".format(name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the raw signal data for Deep learning\n",
    "Here we get the data on the proper format to train deep learning algorithms. In this format every row is acoustic signal with 150000 observations before the event happens. We will get 1'600.000 ys values for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caller_cls = load_object(os.path.join(save_class, \"caller_cls.pkl\"))\n",
    "events_id = [0, 5656573, 50085877, 104677355, 138772452, 187641819, 218652629, 245829584,\n",
    "          307838916, 338276286, 375377847, 419368879, 461811622, 495800224, 528777114,\n",
    "          585568143, 621985672]\n",
    "number_chuncks = 40 #  Number of partitions for our dataset\n",
    "\n",
    "index_list = np.linspace(events_id[0], events_id[-1:], number_chuncks).astype(int)\n",
    "tuples = [(x, y - 150000) for x, y in zip(index_list, index_list[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_0, i_n in tuples:\n",
    "    name = str(i_0) + '_' + str(i_n)\n",
    "    new_data = None\n",
    "    new_data = create_signal_dataset(caller_cl=caller_cls,\n",
    "                                     step=4096,\n",
    "                                     window_size=150000,\n",
    "                                     events_id=(i_0, i_n))\n",
    "    new_data.to_pickle(os.path.join(data_dp, \"signal_s5000_{}.pkl\".format(name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Some experiments for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir(data_cycle) if isfile(join(data_cycle, f))]\n",
    "print('there are {} cycles to train the model'.format(len(onlyfiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.DataFrame([])\n",
    "data_val = pd.DataFrame([])\n",
    "n_train = 14 # 13 only one dataset to test\n",
    "for i, name in enumerate(onlyfiles):\n",
    "    if i <= n_train:\n",
    "        data_train = pd.concat([data_train,\n",
    "                                pd.read_pickle(os.path.join(data_cycle, name))])\n",
    "    else:\n",
    "        data_val = pd.concat([data_train,\n",
    "                                pd.read_pickle(os.path.join(data_cycle, name))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid  = data_train, data_val, data_train[\"time_to_failure\"], data_val[\"time_to_failure\"]\n",
    "del X_train[\"time_to_failure\"]\n",
    "del X_valid[\"time_to_failure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {'num_leaves': 54,\n",
    "              'min_data_in_leaf': 79,\n",
    "              'objective': 'huber',\n",
    "              'max_depth': -1,\n",
    "              'learning_rate': 0.01,\n",
    "              \"boosting\": \"gbdt\",\n",
    "              \"bagging_freq\": 5,\n",
    "              \"bagging_fraction\": 0.8126672064208567,\n",
    "              \"bagging_seed\": 11,\n",
    "              \"metric\": 'mae',\n",
    "              'eval_metric': 'mae',\n",
    "              'reg_alpha': 0.1302650970728192,\n",
    "              'reg_lambda': 0.3603427518866501,\n",
    "              'num_boost_round':20000,\n",
    "              'early_stopping_rounds':10,\n",
    "              'tree_method':'hist'}\n",
    "\n",
    "model = XGBRegressor(params=params_xgb,\n",
    "                     verbose=False,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "# model = LinearRegression(normalize=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "         eval_set = ((X_train, y_train), (X_valid, y_valid))) # This parametrization avoids overfit\n",
    "\n",
    "scores = -1 * cross_val_score(model, X_valid, y_valid, cv=3, scoring='neg_mean_absolute_error')\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99]\n",
    "big = [100, 200, 500, 1000]\n",
    "stalta = [(50, 1000), (100, 1500), (500, 5000), (1000, 10000), (5000, 15000), (10000, 25000)]\n",
    "stalta_window = [(50, 1000), (100, 1500), (500, 5000), (1000, 5000)]\n",
    "exp_mov_ave = [300, 3000, 10000]\n",
    "exp_mov_ave_window = [300, 1000, 2000]\n",
    "\n",
    "computer = FeatureComputer(quantiles=q, abs_quantiles=q, count_abs_big=big, stalta=stalta, stalta_window=stalta_window,\n",
    "                           exp_mov_ave=exp_mov_ave, exp_mov_ave_window=exp_mov_ave_window, window=15000)\n",
    "stft_computer = FeatureComputer(quantiles=q, abs_quantiles=q, count_abs_big=big) # no windows, STALTA, and exp_mov_ave for stft\n",
    "\n",
    "submission = predict_on_test(model=model,\n",
    "                             feature_computer=computer,\n",
    "                             stft_feature_computer=stft_computer,\n",
    "                             ycol=\"time_to_failure\",\n",
    "                             stft=True,\n",
    "                             data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(data_dir, \"submissions\", \"submission_full_cycles_XGBoost_extened_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare distributions\n",
    "Here we check if the data distributions of the features on the train and test are the same Kolmogorov-Smirnov test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load featues of the test set\n",
    "x_test = pickle.load(open(os.path.join(data_dir, \"x_test.pickle\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"all_mad\", \"all_0.95-quantile\", \"all_0.95-abs_quantile\", \"all_hilbert\"]\n",
    "feature_names = x_test.columns[0:5]\n",
    "check_distributions_train_test(x_train=data_train[feature_names],\n",
    "                               x_test=x_test[feature_names],\n",
    "                               n_col=2,\n",
    "                               alpha=0.05,\n",
    "                               density=True,\n",
    "                               bins=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
