{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Experimenting with sparse and dense features\n",
    "\n",
    "In this notebook I will try to experiment with different strategies to handle the combination of sparse and dense data.\n",
    "This is needed because features created using BOW-kind preprocessing tools like `tf_idf` will by design be extremely sparse. On the other hand, artificially constructed features are typically dense. We have identified the following general strategies to tackle this issue:\n",
    "\n",
    "1. Use models robust to many features of varied density and just feed with the concatenation of all features\n",
    "\n",
    "2. Train different classifiers on the sparse and dense datasets and then ensemble them (stacking/boosting)\n",
    "\n",
    "3. Use dimensionality reduction tools like PCA or autoencoders to combine sparse and dense features into better ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from toxicity.linear_predictor import LogisticPredictor, SVMPredictor\n",
    "from toxicity.tuning import tune\n",
    "from toxicity.utils import TAGS\n",
    "from common.nlp.preprocessing import tf_idf\n",
    "from common.nlp.feature_adder import FeatureAdder\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "train = pd.read_csv(data_dir + \"train.csv\")\n",
    "test = pd.read_csv(data_dir + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_ys = {tag: train[tag].values for tag in TAGS}\n",
    "\n",
    "# Get the sparse dataset\n",
    "sparse_train, sparse_test = tf_idf(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictor = LogisticPredictor(C=4)\n",
    "predictor.evaluate(sparse_train, train_ys, method='CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the dense features\n",
    "fa_params = {\n",
    "    \"data_dir\": data_dir,\n",
    "    \"upper_case\": True,\n",
    "    \"word_count\": True,\n",
    "    \"unique_words_count\": True,\n",
    "    \"letter_count\": True,\n",
    "    \"punctuation_count\": True,\n",
    "    \"little_case\": True,\n",
    "    \"stopwords\": True,\n",
    "    \"question_or_exclamation\": True,\n",
    "    \"number_bad_words\": True\n",
    "}\n",
    "fa = FeatureAdder(**fa_params)\n",
    "    \n",
    "dense_train, dense_test = fa.add_features(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing Predictors\n",
    "\n",
    "We now have both the sparse and dense feature sets. Lets explore their predictive power using our model arsenal.\n",
    "We will use default parameters for now, but each of those predictor must be tuned to reach its full potential.\n",
    "Let's test all predictors on both the dense and sparse datasets, as different predictors are expected to perform better for different input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classes = [SVMPredictor, RandomForestPredictor, LightGBMPredictor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test dense features\n",
    "for cls in classes:\n",
    "    predictor = cls()  # Create an object using default parameters - probably suboptimal\n",
    "    predictor.evaluate(dense_train, train_ys, val_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test sparse features\n",
    "for cls in classes:\n",
    "    predictor = cls() # Create an object using default parameters - probably suboptimal\n",
    "    predictor.evaluate(sparse_train, train_ys, val_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
