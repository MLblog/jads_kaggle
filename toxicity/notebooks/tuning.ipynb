{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning models\n",
    "\n",
    "In this notebook we show how the tuning library can be called for any predictor implementing our `fit` and `evaluate` methods.\n",
    "The tuning process can be differentiated based on a number of different settings, most importantly the set of parameters to be explored and the method of evaluation. The latter is by default set to train-test split for timing reasons (since GridSearch is a computationally expensive procedure). However more robust results are likely using `method='CV'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from linear_predictor import LogisticPredictor, SVMPredictor\n",
    "from tuning import tune, bayesian_optimization\n",
    "from utils import TAGS\n",
    "from preprocessing import tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Preprocess raw text data\n",
    "train_ys = {tag: train[tag].values for tag in TAGS}\n",
    "train_x, test_x, _ = tf_idf(train, test)\n",
    "\n",
    "# Define output file\n",
    "write_to = '../data/tuning.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets run the tuner using sample parameters choices.\n",
    "\n",
    "We can optionally persist our results to a file for later inspection. This was we will not have to check the same parameter sets again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [4, 5],\n",
    "    'dual': [True, False]\n",
    "}\n",
    "\n",
    "best_params, best_score = tune(LogisticPredictor, train_x, train_ys, param_grid, silent=False, persist=False, write_to=write_to)\n",
    "print(\"Optimal parameters achieve log loss = {}\".format(best_score))\n",
    "print(\"Optimal 'C': {}\\nOptimal 'dual': {}\".format(best_params['C'], best_params['dual']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing linear predictors\n",
    "\n",
    "How do our classifiers perform in comparison to each other? Let's try them both with minimal tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [5, 6],\n",
    "    'dual': [True, False]\n",
    "}\n",
    "\n",
    "best_params, best_score = tune(SVMPredictor, train_x, train_ys, param_grid, silent=False, persist=True, write_to=write_to)\n",
    "print(\"Optimal parameters achieve log loss = {}\".format(best_score))\n",
    "print(\"Optimal 'C': {}\\nOptimal 'dual': {}\".format(best_params['C'], best_params['dual']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The linear SVM Predictor seems to outperform logistic regression as it is not only faster but also coming up with a better log loss. **This could change given proper tuning however.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization\n",
    "\n",
    "The tuning library contains an implementation of Bayesian Optimization using the GPyOpt package. Inputs are similar as above, complemented with the following:\n",
    "\n",
    "1. The parameter bounds are specified as a list of dictionaries, where each dict specifies the name, type (\"continuous\" or \"discrete\"), and the domain of the variable. Note, continuous variables must be specified before discrete ones. Example:\n",
    "\n",
    "2. The maximum number of iterations or the maximum allowed time needs to be specified.\n",
    "\n",
    "3. The model type is either a Gaussian Process ('GP'), ...,\n",
    "\n",
    "4. Acquisition function determines the next set of parameters to be evaluated.\n",
    "\n",
    "5. Acquisition weight is used to define the balance between exploration and exploitation. \n",
    "\n",
    "Let's test the tuner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{\"name\": \"C\", \"type\":\"continuous\", \"domain\": (0.1, 6.0)},\n",
    "          {\"name\": \"dual\", \"type\":\"discrete\", \"domain\": [False, True]}]\n",
    "\n",
    "best_params, best_score = bayesian_optimization(SVMPredictor, train_x, train_ys, params, model_type='GP', acquisition_type='EI', \n",
    "                              acquisition_weight=2, max_iter=10, max_time=None, silent=True, persist=False, write_to=write_to)\n",
    "\n",
    "print(\"Optimal parameters achieve log loss = {}\".format(best_score))\n",
    "print(\"Optimal 'C': {}\\nOptimal 'dual': {}\".format(best_params['C'], best_params['dual']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Bayesian Optimization let's us easily try a wider range of settings at lower costs (time). In fact, we can now evaluate continuous variables on a continuous scale, rather than choosing a fixed range of values. \n",
    "\n",
    "Note that at this moment, no parallel processing is implemented for Bayesian Optimization. This would greatly improve the efficiency of this approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
