{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Quora dataset"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"dark_background\") # comment out if using light Jupyter theme\n",
    "\n",
    "dtypes = {\"qid\": str, \"question_text\": str, \"target\": int}\n",
    "train = pd.read_csv(\"../data/train.csv\", dtype=dtypes)\n",
    "test = pd.read_csv(\"../data/test.csv\", dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A first glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} questions in train and {} in test\".format(train.shape[0], test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target value is binary (values: {})\".format(set(train[\"target\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of toxic questions in training data is {} (proportion: {}).\".format(train[\"target\"].sum(), train[\"target\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A closer look at the questions\n",
    "\n",
    "### 2.1 Question length (characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text_length\"] = train[\"question_text\"].str.len()\n",
    "train[\"text_length\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most questions are relatively short, i.e., less than 100 characters. There are some exceptions, however, with a maximum of more than a thousand. Let's see how many characters we should consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for length in [100, 150, 200, 250, 300, 350, 500]:\n",
    "    num = np.sum(train[\"text_length\"] > length)\n",
    "    print(\"There are {} questions ({}%) with more than {} characters.\"\n",
    "          .format(num, np.round(num / len(train) * 100, 2), length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of questions with more than 250 characters is already small and with more than 300 negligible. We can cut the questions at 300 or even just remove them. Would there be a difference between the length of toxic and sincere questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_target(data):\n",
    "    toxic = data[data[\"target\"] == 1]\n",
    "    sincere = data[data[\"target\"] == 0]\n",
    "    return sincere, toxic\n",
    "\n",
    "sincere, toxic = split_on_target(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_plots(sincere_data, toxic_data, column, xlim=(0, 300), bin_size=5):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axes[0] = sns.distplot(sincere_data[column], ax=axes[0], bins=np.arange(xlim[0], xlim[1], bin_size))\n",
    "    axes[0].set_title(\"Sincere questions\")\n",
    "    axes[1] = sns.distplot(toxic_data[column], ax=axes[1], bins=np.arange(xlim[0], xlim[1], bin_size))\n",
    "    axes[1].set_title(\"Toxic questions\")\n",
    "    if xlim is not None:\n",
    "        for ax in axes:\n",
    "            ax.set_xlim(xlim[0], xlim[1])\n",
    "    plt.suptitle(\"Comparison of {} between sincere and toxic questions\".format(column))\n",
    "    plt.show()\n",
    "\n",
    "plot_density_plots(sincere, toxic, \"text_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toxic questions seem to have a higher chance of having somewhat more characters, although the medians seem to be more or less the same. The numbers confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([sincere[\"text_length\"].describe(), toxic[\"text_length\"].describe()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Question length (words)\n",
    "A similar analysis can be done based on the number of _words_ per question, rather than the number of characters. To do this properly, we should probably first remove symbols and punctuation, but let's take a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"words\"] = train[\"question_text\"].apply(lambda x: len(x.split(\" \")))\n",
    "sincere, toxic = split_on_target(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_plots(sincere, toxic, \"words\", xlim=(0, 60), bin_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same conclusion seems to hold for the number of words. It is, thus, useful to include the question size as a feature in our models. Also, it seems that there are not many questions with more than 50 or 60 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [50, 55, 60]:\n",
    "    print(\"{} questions with more than {} words.\".format(np.sum(train[\"words\"] > n), n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
