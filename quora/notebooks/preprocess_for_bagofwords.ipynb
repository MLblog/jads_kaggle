{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data for the bag of words approach\n",
    "\n",
    "This notebooks performs preprocessing on the Quora datasets for the bag of words approach. This means that we do not take into account the text or sequence of words itself, but information such as punctuation, length, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from common.nlp.feature_adder import FeatureAdder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data with right data types (this is important for the IDs in particular)\n",
    "dtypes = {\"qid\": str, \"question_text\": str, \"target\": int}\n",
    "train = pd.read_csv(\"../data/train.csv\", dtype=dtypes)\n",
    "test = pd.read_csv(\"../data/test.csv\", dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features to the datasets\n",
    "The `common.nlp.feature_adder` module is used to subtract features from the text. This takes around 20 minutes. Therefore, it could be beneficial to evaluate with experiments which features are usefull and which are not.\n",
    "\n",
    "Since we cannot external data, we define a list of badwords ourselves. In the section 'Create a list of bad words' it is explained how we get to this list of bad words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badwords = ['ahole', 'asshole', 'bareback', 'bastard', 'beastial', 'bestial', 'big black', 'bitch', 'black cock', 'chink', \n",
    "            'cocks', 'creampie', 'cunt', 'dick', 'feck', 'fondle', 'fuc', 'gays', 'golden shower', 'incest', 'jackass', 'lesbians',\n",
    "            'lusty', 'moron', 'pedophilia', 'pricks', 'puss', 'raped', 'raping', 'scum', 'shit', 'sissy', 'sluts', 'sodom', 'tits', \n",
    "            'tranny', 'transsexual', 'whore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dense features\n",
    "fa_params = {\n",
    "    \"data_dir\": 'Data/',\n",
    "    \"upper_case\": True,\n",
    "    \"word_count\": True,\n",
    "    \"unique_words_count\": True,\n",
    "    \"letter_count\": True,\n",
    "    \"punctuation_count\": True,\n",
    "    \"little_case\": True,\n",
    "    \"stopwords\": True, #this is using a list which is in the nltk package (external data), so I dont think we can use it.\n",
    "    \"question_or_exclamation\": True,\n",
    "    \"number_bad_words\": True,\n",
    "    \"sentiment_analysis\": True,\n",
    "    \"badwords\": badwords,\n",
    "    \"text_column\": \"question_text\"\n",
    "}\n",
    "fa = FeatureAdder(**fa_params)\n",
    "\n",
    "train_extended, test_extended = fa.get_features(train, test, load = False, save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of bad words\n",
    "Since we cannot use external data it is not possible to use an list of bad words from the internet. What we can do it create one ourselve with one the most important bad words. The main question here is: how do we define the most important bad words for this competition. \n",
    "\n",
    "We start with a list of bad words that is used by google. Then we perform the following steps:\n",
    "- For each google bad word, we count how many times it occurs and how many times the text in which the word occurs is classified as insincere.  \n",
    "- Select only bad words that occur in the train set and are classified as insincere in more than X% of the cases\n",
    "- Drop duplicate bad words \n",
    "- Select only words that are not already captured by another selected bad word. \n",
    "- Keep only bad words that occur in the test dataset\n",
    "\n",
    "The selected words\n",
    "\n",
    "NOTE! It takes around 30 minutes to run this code. So, I recommend you to test it on a part of the dataset or with only a part of the google bad words. The resulting badwords_quora.pkl file can be found in the shared surfdrive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_badwords = pd.read_csv('../data/google_badwords.csv', encoding = 'ISO-8859-1', header = None, sep = ';')[[0]]\n",
    "# load data with right data types (this is important for the IDs in particular)\n",
    "dtypes = {\"qid\": str, \"question_text\": str, \"target\": int}\n",
    "train = pd.read_csv(\"../data/train.csv\", dtype=dtypes)\n",
    "test = pd.read_csv(\"../data/test.csv\", dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count how many times all words are occuring in the train set and the percentage classified as insincere\n",
    "occurance = [] # A list with for each google bad word the number of times it occurs in the training set\n",
    "classification = [] # A list with for each google bad words the probability of being classified as insincere\n",
    "\n",
    "for word in google_badwords[0]:\n",
    "    # Check for each row if the word occurs\n",
    "    occur = train['question_text'].apply(lambda x: word in x)\n",
    "    # Select only the data in which the word occurs\n",
    "    select = occur[occur == True]\n",
    "    \n",
    "    if len(select) > 0: \n",
    "        index = select.index\n",
    "        occurance.append(len(index))\n",
    "        classification.append(list(train.iloc[index,:]['target']).count(1)/len(index))\n",
    "    else:\n",
    "        occurance.append(0)\n",
    "        classification.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_badwords['occurance'] = occurance\n",
    "google_badwords['%insincere'] = classification\n",
    "google_badwords.to_pickle('data/badwords_quora.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file\n",
    "google_badwords = pd.read_pickle('data/badwords_quora.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only words that are classified often as insincere\n",
    "selected_words_train = list(google_badwords[(google_badwords['occurance'] >= 0) & (google_badwords['%insincere'] > 0.55)][0])\n",
    "print(len(selected_words_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate words\n",
    "selected_words_train = list(set(selected_words_train))\n",
    "print(len(selected_words_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only words that are not already captured by another selected word. \n",
    "# For example 'motherfuck' and 'motherfucking' are amongst the list of bad words. Then 'motherfucking' is redundant, \n",
    "# since 'motherfucking' is already captured by 'motherfuck'\n",
    "remove_words = []\n",
    "for word1 in selected_words_train:\n",
    "    for word2 in selected_words_train:\n",
    "        if word1 != word2:\n",
    "            if word1 in word2:\n",
    "                remove_words.append(word2)\n",
    "selected_words_train = list(set(selected_words_train) - set(remove_words))\n",
    "print(len(selected_words_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only bad words that occur in the test dataset\n",
    "selected_words = []\n",
    "for word in selected_words_train:\n",
    "    occur = test['question_text'].apply(lambda x: word in x)\n",
    "    select = occur[occur == True]\n",
    "    if len(select) > 0: \n",
    "        selected_words.append(word)\n",
    "print(len(selected_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the words alphabeticaly\n",
    "selected_words.sort()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
